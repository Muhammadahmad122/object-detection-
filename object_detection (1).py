# -*- coding: utf-8 -*-
"""object detection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GFI-6ZHq4_9CeiXT7AHXVamKQKLL0SLj
"""

!pip install -q --upgrade keras-cv
!pip install -q --upgrade keras

import os

os.environ["KERAS_BACKEND"] = "jax"  # @param ["tensorflow", "jax", "torch"]

from tensorflow import data as tf_data
import tensorflow_datasets as tfds
import keras
import keras_cv
import numpy as np
from keras_cv import bounding_box
import os
from keras_cv import visualization
import tqdm
import tensorflow as tf

pretrained_model = keras_cv.models.YOLOV8Detector.from_preset(
    "yolo_v8_m_pascalvoc", bounding_box_format="xywh"
)

from google.colab import drive
drive.mount('/content/drive')

# Load image from Google Drive
filepath = '/content/drive/MyDrive/object.jpeg'
image = keras.utils.load_img(filepath)
image = np.array(image)

# Plot the image gallery
visualization.plot_image_gallery(
    np.array([image]),
    value_range=(0, 255),
    rows=1,
    cols=1,
    scale=5
)

inference_resizing = keras_cv.layers.Resizing(
    640, 640, pad_to_aspect_ratio=True, bounding_box_format="xywh"
)

image_batch = inference_resizing([image])

class_ids = [
    "Aeroplane",
    "Bicycle",
    "Bird",
    "Boat",
    "Bottle",
    "Bus",
    "Car",
    "Cat",
    "Chair",
    "Cow",
    "Dining Table",
    "Dog",
    "Horse",
    "Motorbike",
    "Person",
    "Potted Plant",
    "Sheep",
    "Sofa",
    "Train",
    "Tvmonitor",
    "Total",
]
class_mapping = dict(zip(range(len(class_ids)), class_ids))

y_pred = pretrained_model.predict(image_batch)
# y_pred is a bounding box Tensor:
# {"classes": ..., boxes": ...}
visualization.plot_bounding_box_gallery(
    image_batch,
    value_range=(0, 255),
    rows=1,
    cols=1,
    y_pred=y_pred,
    scale=5,
    font_scale=0.7,
    bounding_box_format="xywh",
    class_mapping=class_mapping,
)

# The following NonMaxSuppression layer is equivalent to disabling the operation
prediction_decoder = keras_cv.layers.NonMaxSuppression(
    bounding_box_format="xywh",
    from_logits=True,
    iou_threshold=1.0,
    confidence_threshold=0.0,
)
pretrained_model = keras_cv.models.YOLOV8Detector.from_preset(
    "yolo_v8_m_pascalvoc",
    bounding_box_format="xywh",
    prediction_decoder=prediction_decoder,
)

y_pred = pretrained_model.predict(image_batch)
visualization.plot_bounding_box_gallery(
    image_batch,
    value_range=(0, 255),
    rows=1,
    cols=1,
    y_pred=y_pred,
    scale=5,
    font_scale=0.7,
    bounding_box_format="xywh",
    class_mapping=class_mapping,
)

prediction_decoder = keras_cv.layers.NonMaxSuppression(
    bounding_box_format="xywh",
    from_logits=True,
    # Decrease the required threshold to make predictions get pruned out
    iou_threshold=0.2,
    # Tune confidence threshold for predictions to pass NMS
    confidence_threshold=0.7,
)
pretrained_model = keras_cv.models.YOLOV8Detector.from_preset(
    "yolo_v8_m_pascalvoc",
    bounding_box_format="xywh",
    prediction_decoder=prediction_decoder,
)

y_pred = pretrained_model.predict(image_batch)
visualization.plot_bounding_box_gallery(
    image_batch,
    value_range=(0, 255),
    rows=1,
    cols=1,
    y_pred=y_pred,
    scale=5,
    font_scale=0.7,
    bounding_box_format="xywh",
    class_mapping=class_mapping,
)

"""# Advance

Before we had tried through pretrained model <br>
Now we are going to train model and make predictions through it

# Data Loading
"""

import tensorflow as tf

BATCH_SIZE = 4
batch = 4
num_boxes = 4

bounding_boxes = {
    # Initialize the 'boxes' with zeros (4 coordinates for each box)
    'boxes': tf.zeros([batch, num_boxes, 4]),
    # Initialize the 'classes' with zeros
    'classes': tf.zeros([batch, num_boxes])
}

def visualize_dataset(inputs, value_range, rows, cols, bounding_box_format):
    inputs = next(iter(inputs.take(1)))
    images, bounding_boxes = inputs["images"], inputs["bounding_boxes"]
    visualization.plot_bounding_box_gallery(
        images,
        value_range=value_range,
        rows=rows,
        cols=cols,
        y_true=bounding_boxes,
        scale=5,
        font_scale=0.7,
        bounding_box_format=bounding_box_format,
        class_mapping=class_mapping,
    )


def unpackage_raw_tfds_inputs(inputs, bounding_box_format):
    image = inputs["image"]
    boxes = keras_cv.bounding_box.convert_format(
        inputs["objects"]["bbox"],
        images=image,
        source="rel_yxyx",
        target=bounding_box_format,
    )
    bounding_boxes = {
        "classes": inputs["objects"]["label"],
        "boxes": boxes,
    }
    return {"images": image, "bounding_boxes": bounding_boxes}


def load_pascal_voc(split, dataset, bounding_box_format):
    ds = tfds.load(dataset, split=split, with_info=False, shuffle_files=True)
    ds = ds.map(
        lambda x: unpackage_raw_tfds_inputs(x, bounding_box_format=bounding_box_format),
        num_parallel_calls=tf_data.AUTOTUNE,
    )
    return ds

train_ds = load_pascal_voc(
    split="train", dataset="voc/2007", bounding_box_format="xywh"
)
eval_ds = load_pascal_voc(split="test", dataset="voc/2007", bounding_box_format="xywh")

train_ds = train_ds.shuffle(BATCH_SIZE * 4)

train_ds = train_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)
eval_ds = eval_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)

visualize_dataset(
    train_ds, bounding_box_format="xywh", value_range=(0, 255), rows=2, cols=2
)

visualize_dataset(
    eval_ds,
    bounding_box_format="xywh",
    value_range=(0, 255),
    rows=2,
    cols=2,
    # If you are not running your experiment on a local machine, you can also
    # make `visualize_dataset()` dump the plot to a file using `path`:
    # path="eval.png"
)

"""# Data augmentation

"""

augmenters = [
    keras_cv.layers.RandomFlip(mode="horizontal", bounding_box_format="xywh"),
    keras_cv.layers.JitteredResize(
        target_size=(640, 640), scale_factor=(0.75, 1.3), bounding_box_format="xywh"
    ),
]


def create_augmenter_fn(augmenters):
    def augmenter_fn(inputs):
        for augmenter in augmenters:
            inputs = augmenter(inputs)
        return inputs

    return augmenter_fn


augmenter_fn = create_augmenter_fn(augmenters)

train_ds = train_ds.map(augmenter_fn, num_parallel_calls=tf_data.AUTOTUNE)
visualize_dataset(
    train_ds, bounding_box_format="xywh", value_range=(0, 255), rows=2, cols=2
)

inference_resizing = keras_cv.layers.Resizing(
    640, 640, bounding_box_format="xywh", pad_to_aspect_ratio=True
)
eval_ds = eval_ds.map(inference_resizing, num_parallel_calls=tf_data.AUTOTUNE)

visualize_dataset(
    eval_ds, bounding_box_format="xywh", value_range=(0, 255), rows=2, cols=2
)

def dict_to_tuple(inputs):
    return inputs["images"], bounding_box.to_dense(
        inputs["bounding_boxes"], max_boxes=10
    )


train_ds = train_ds.map(dict_to_tuple, num_parallel_calls=tf_data.AUTOTUNE)
eval_ds = eval_ds.map(dict_to_tuple, num_parallel_calls=tf_data.AUTOTUNE)

train_ds = train_ds.prefetch(tf_data.AUTOTUNE)
eval_ds = eval_ds.prefetch(tf_data.AUTOTUNE)

"""# Optimizer"""

base_lr = 0.005
# including a global_clipnorm is extremely important in object detection tasks
optimizer = keras.optimizers.SGD(
    learning_rate=base_lr, momentum=0.9, global_clipnorm=10.0
)

"""# Loss functions"""

pretrained_model.compile(
    classification_loss="binary_crossentropy",
    box_loss="ciou",
)

"""# Metric evaluation"""

coco_metrics_callback = keras_cv.callbacks.PyCOCOCallback(
    eval_ds.take(20), bounding_box_format="xywh"
)

"""# Model creation"""

model = keras_cv.models.YOLOV8Detector.from_preset(
    "resnet50_imagenet",
    # For more info on supported bounding box formats, visit
    # https://keras.io/api/keras_cv/bounding_box/
    bounding_box_format="xywh",
    num_classes=20,
)

"""# Model Training"""

model.compile(
    classification_loss="binary_crossentropy",
    box_loss="ciou",
    optimizer=optimizer,
)

print("Train Dataset:", train_ds)
print("Evaluation Dataset:", eval_ds)
print("Callback:", coco_metrics_callback)

model.fit(
    train_ds.take(20),
    # Run for 10-35~ epochs to achieve good scores.
    epochs=35,
    validation_data=eval_ds.take(20),
    callbacks=[coco_metrics_callback],
)

model.save("yolo.keras")



"""# Inference and plotting results"""



# model = keras_cv.models.YOLOV8Detector.from_preset(
#    "yolo_v8_m_pascalvoc", bounding_box_format="xywh"
# )

# import h5py

# with h5py.File(pretrained_model_path, 'r') as f:
#     model = load_model(f)

# from google.colab import drive
# from keras.models import load_model
# from keras.utils import custom_object_scope
# from keras_cv.models import YOLOV8Detector

# # Mount Google Drive
# # drive.mount('/content/gdrive')

# # Register the YOLOV8Detector layer as a custom object
# with custom_object_scope({'YOLOV8Detector': YOLOV8Detector, 'CIoULoss': CIoULoss}):
#     # Load the pre-trained model from the file on Google Drive
#     pretrained_model_path = '/content/drive/MyDrive/yolo.h5'
#     model = load_model(pretrained_model_path)

# # Set the bounding box format
# model.bounding_box_format = 'xywh'

# from tensorflow import keras
# import tensorflow as tf
# import keras_cv
# from keras_cv.src.losses.ciou_loss import CIoULoss

# ciou_loss = CIoULoss(bounding_box_format='xyxy')
# # Define the custom objects, including CIoULoss with bounding_box_format
# custom_objects = {
#     "YOLOV8Detector": keras_cv.models.YOLOV8Detector,
#     "CIoULoss": lambda: keras_cv.losses.CIoULoss(bounding_box_format="xywh")
# }

# # Load the model using custom object scope
# model_path = '/content/drive/My Drive/yolo.h5'
# model = keras.models.load_model(model_path, custom_objects=custom_objects)

visualization_ds = eval_ds.unbatch()
visualization_ds = visualization_ds.ragged_batch(4)
visualization_ds = visualization_ds.shuffle(8)

def visualize_detections(model, dataset, bounding_box_format):
    images, y_true = next(iter(dataset.take(1)))
    y_pred = model.predict(images)
    visualization.plot_bounding_box_gallery(
        images,
        value_range=(0, 255),
        bounding_box_format=bounding_box_format,
        y_true=y_true,
        y_pred=y_pred,
        scale=4,
        rows=2,
        cols=2,
        show=True,
        font_scale=0.7,
        class_mapping=class_mapping,
    )

model.prediction_decoder = keras_cv.layers.NonMaxSuppression(
    bounding_box_format="xywh",
    from_logits=True,
    iou_threshold=0.5,
    confidence_threshold=0.75,
)

visualize_detections(model, dataset=visualization_ds, bounding_box_format="xywh")

class VisualizeDetections(keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs):
        visualize_detections(
            self.model, bounding_box_format="xywh", dataset=visualization_ds
        )

model.prediction_decoder = keras_cv.layers.NonMaxSuppression(
    bounding_box_format="xywh",
    from_logits=True,
    iou_threshold=0.3,  # Lower IoU threshold
    confidence_threshold=0.2  # Lower confidence threshold
)

# Extract the labels (ground truth) from visualization_ds
for images, labels in visualization_ds.take(1):  # Take one batch for an example
    y_true = labels  # This should be the ground truth labels

def calculate_iou(box1, box2):
    """Compute IoU between two bounding boxes."""
    box1 = np.array(box1)
    box2 = np.array(box2)

    # Extract xmin, ymin, xmax, ymax from both boxes
    x1_min, y1_min, x1_max, y1_max = box1[:4]
    x2_min, y2_min, x2_max, y2_max = box2[:4]

    # Compute the intersection coordinates
    inter_xmin = np.maximum(x1_min, x2_min)
    inter_ymin = np.maximum(y1_min, y2_min)
    inter_xmax = np.minimum(x1_max, x2_max)
    inter_ymax = np.minimum(y1_max, y2_max)

    # Compute the area of intersection
    inter_area = np.maximum(0, inter_xmax - inter_xmin) * np.maximum(0, inter_ymax - inter_ymin)

    # Compute the area of both bounding boxes
    box1_area = (x1_max - x1_min) * (y1_max - y1_min)
    box2_area = (x2_max - x2_min) * (y2_max - y2_min)

    # Compute the union area
    union_area = box1_area + box2_area - inter_area

    # Compute the IoU
    iou = np.divide(inter_area, union_area, out=np.zeros_like(inter_area), where=union_area != 0)

    return iou

# Define the function to convert bounding box formats
def xywh_to_xyxy(box):
    x, y, w, h = box[:4]
    xmin = x - w / 2
    ymin = y - h / 2
    xmax = x + w / 2
    ymax = y + h / 2
    return [xmin, ymin, xmax, ymax]

# Assuming `y_pred` contains the predicted boxes after running the model on images
for images, labels in visualization_ds.take(1):
    y_true = labels
    y_pred = model.predict(images)

    # Assuming `y_pred['boxes']` contains the predicted boxes
    for pred_box, true_box in zip(y_pred['boxes'], y_true['boxes']):
        pred_box_xyxy = xywh_to_xyxy(pred_box)  # Convert predicted box to xyxy format
        iou = calculate_iou(pred_box_xyxy, true_box)  # Calculate IoU between predicted and true box
        print(f"IoU between predicted box {pred_box_xyxy} and true box {true_box} is {iou}")

# # Set a batch size for the validation dataset
# BATCH_SIZE = 4  # You can adjust the batch size as per your requirement

# # Apply batching to the visualization_ds
# batched_visualization_ds = visualization_ds.batch(BATCH_SIZE)

# # Use the batched dataset with the PyCOCOCallback
# coco_evaluator = PyCOCOCallback(
#     validation_data=batched_visualization_ds,  # Use the batched dataset
#     bounding_box_format="xywh"
# )

# # Assuming your model is already set
# coco_evaluator.set_model(model)

# # Run the evaluation at the end of an epoch
# coco_evaluator.on_epoch_end(epoch=1, logs={})

from keras_cv.callbacks import PyCOCOCallback

# Assuming visualization_ds is your validation dataset
coco_evaluator = PyCOCOCallback(
    validation_data=visualization_ds.take(20),  # Your validation dataset
    bounding_box_format="xywh"
)

coco_evaluator.set_model(model)

# Manually run the evaluation after training
coco_evaluator.on_epoch_end(epoch=1, logs={})

